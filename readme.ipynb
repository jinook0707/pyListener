{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyListener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyListener is an open-source software written in Python for sound comparison.<br>\n",
    "Currently, its main functionality is to listen to sound from microphone and save a recognized sound as WAV file, when it is similar with a loaded template sound.\n",
    "\n",
    "Jinook Oh, Cognitive Biology department, University of Vienna<br>\n",
    "Contact: jinook0707@gmail.com, tecumseh.fitch@univie.ac.at<br>\n",
    "September 2019.\n",
    "\n",
    "\n",
    "<br>A video file, 'pyListenerDemo.mp4', shows an examplar usage case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"pyListenerDemo.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What it does\n",
    "\n",
    "In short, PyListener app does the below.<br>\n",
    "**1)** Load template WAV file(s) to analyze & form template WAV data and parameters to compare.<br>\n",
    "**2)** Captures a sound from streaming of microphone.<br>\n",
    "**3)** This captured sound is also analyzed & its result parameters will be compared to those of template WAV data.<br>\n",
    "**4)** Store result text in log file and show it on the app UI. Also, if two sounds match, the captured sound will be saved as a WAV file in 'recordings' folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyListener app, a user can load wave file(s) in a folder to form a template sound, then start/stop listening from a microphone.\n",
    "A sound will be automatically cut out of the continuous listening, when amplitude goes over a threshold and stay longer than another threshold.<br>\n",
    "Related variables are as below (defined in *\\_\\_init\\__* of *pyListener*).<br>\n",
    "- PyListener.**ampMonDur**: Time (in seconds) to look back for average amplitude. This should be longer than PyListener.**minDur4SF**.\n",
    "- PyListener.**minDur4SF**: Minimum duration (in seconds) for a sound fragment.\n",
    "- PyListener.**ampThr**: Amplitude (0.0-1.0) threshold to start a sound fragment.\n",
    "- PyListener.**maxDurLowerThr**: Once amplitude goes above threshold, the program will continute to capture audio data until amplitude is lower than PyListener.**ampThr** longer than this duration, PyListener.**maxDurLowerThr**.\n",
    "\n",
    "Analyzed parameters of both template sound and a sound fragment, captured as above, are used to compare them to determine whether it's similar or not.\n",
    "The below variables are related to define parameters to be analyzed. They are located in *\\_\\_init\\__* function of *pyListener*.<br>\n",
    "- PyListener.**pKeys**: This is a list of parameters to store such as *duration*, *summed amplitude*, *center of mass*, *low frequency*, *high frequency* and so on. Some of these are for the comparision, others are just for calculations in the app. **Description for each item can be found at the end of this document**.\n",
    "- PyListener.**compParamList**: This is a subset list of PyListener.**pKeys** to use in the comparison.\n",
    "- PyListener.**indCPL**: This is a subset list of PyListener.**compParamList**, which does not change when different template WAV files are selected. (Currently, there are only two items, 'summedAmpRatio' and 'corr2auto'.\n",
    "- PyListener.**cplInitMargin**: This is a Python dictionary, which has values to subtract or add to the template WAV parameters. Keys are keys of PyListener.**compParamList** (excluding keys in PyListener.**indCPL**) plus \"\\_min\" and \"\\_max\".\n",
    "- PyListener.**indCPRange**: This is a Python dictionary, which has threshold ranges for items in PyListener.**indCPL**. Keys are keys of PyListener.**indCPL** plus \"\\_min\" and \"\\_max\". Its values are the minimum and maximum values respectively.\n",
    "- PyListener.**compParamLabel**: This is a list of labels to appear in UI of *pyListener*. The items of PyListener.**compParamList** will appear in UI of *pyListerner* for manual adjustments of threshold ranges, and these labels will be shown next to it.\n",
    "\n",
    "\\* When a new parameter to be analyzed is added, the above variables should accordingly updated. Will be explained later in this document.<br>\n",
    "\\** A user can permanently change these variables or temporarily disable certain parameters in PyListener.**compParamList** by unchecking it in UI of pyListner app.\n",
    "\n",
    "The analyzed results will be compared with the analyzed results of the template file.\n",
    "If it's similar enough (within all the thresholding ranges), the app will save the sound fragment as a wave file in a folder named **recordings**. Also, all the results of each analysis will be saved in a log file in **log** folder. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example comparison, using pyListenerLib.py as a library\n",
    "\n",
    "Currently, pyListener has three Python files, <br>\n",
    "- **pyListener.py**: pyListener app, using wxPython.<br>\n",
    "- **pyListenerLib.py**: This contains main functionalities of pyListener such as sound loading, comparing and saving. This can be used without loading wxPython frame in **pyListener.py**.<br>\n",
    "- **fFuncNClasses.py**: Simple functions and a dialog class to be used in multiple places in the above files.\n",
    "\n",
    "Here, we will test sound comparision functionality with **pyListenerLib.py** without wxPython frame.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import necessary packages, then, create _PyListener_ object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd\n",
    "import pyListenerLib as PLL\n",
    "pl = PLL.PyListener(parent=None, frame=None, logFile='log/testLog.txt', cwd=getcwd()) # initialize pyListener class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have wxPython frame in this notebook example, therefore, we give 'None' for both parent and frame.\n",
    "As it's created, it will make a text file, testLog.txt, in 'log' folder in your working directory (assuming your working directory is where pyListern files are) to record all comparison results.\n",
    "\n",
    "When **PyListener** class is intialized, it finds microphones using a preferred device string (PyListener.**prefDevStr**), which is currently set to ['built-in'] in the code. (Because PyListener was programmed on Mac OSX and it has 'built-in' microphone').\n",
    "If you want to use another microphone attached to your computer, you can find input devices again by running the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.prefDevStr = ['H4N', 'built-in'] # in case, you have H4N microphone attached.\n",
    "pl.devIdx, pl.devNames = pl.find_device(devType='input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*devIdx* and *devNames* are lists.\n",
    "If you're not sure what string should be give, please open 'testLog.txt', created on **PyListener**'s initialization.\n",
    "The first few lines will say about input devices, such as 'device-name:Built-in Microphone'.\n",
    "You can give a peculiar string in that name. (Letter case doesn't matter here. pl.prefDevStr=['BUILT-IN'] will also work.)\n",
    "\n",
    "When listening via a microphone is started, a device index can be set to a specific one among found devices.\n",
    "You will see this later in this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading template dataset\n",
    "\n",
    "In this example, we'll try to catch *phee* call type of common marmoset monkeys.<br>\n",
    "We give a folder path which contains several WAV files of *phee* call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSpAD, __, templP = pl.listen(flag='templateFolder', wavFP='input/sample_phee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyListener.**listen** function reads and analyzes WAV files with a given folder path, *wavFP*,<br>\n",
    " and it returns **data**, **amp** and **params**.\n",
    "\n",
    "**data** is a numpy array (of spectrgoram) which contains greyscale pixel values (0-255) for drawing a spectrogram.<br>\n",
    "**amp** is a RMS amplitude which is to check whether the amplitude of real time recording sound is over/under threshold to determine beginning/end of a sound fragment. But, it's not necessary here since we'll deal with already prepared WAV files.<br>\n",
    "**params** is a Python dictionary containing analyzed parameters of the given sound data. When it's a template folder (instead of a sound fragment of real-time listening), it has average values of WAV files in the folder. Also, it has '*parameter*\\_min' and '*parameter*\\_max' to show initial threshold ranges.<br>If you need what is actually calculated to generate this params, see functions called in *analyzeSpectrogramArray* function in *PyListener*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running comparison with WAV files in testing folder\n",
    "\n",
    "Here, instead of capturing a sound fragment in real-time listening, we're going through test WAV files to compare them with the template file parameters (*templP*). The test WAV files starts with 'm_' in *input/test* folder; they're manually cut different marmoset calls such as *phee*, *rapid fire tsik*, *etc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os import path\n",
    "\n",
    "### Prepare min. and max. values for thresholding.\n",
    "### In the app, this process will be done, using values in UI (TextCtrl widgets).\n",
    "tParams = {}\n",
    "for param in pl.compParamList:\n",
    "    tParams[param+'_min'] = templP[param+\"_min\"]\n",
    "    tParams[param+'_max'] = templP[param+\"_max\"]\n",
    "\n",
    "fLists = glob('input/test/m_*.wav')\n",
    "for fp in sorted(fLists): # loop through WAV files\n",
    "    spAD, __, sfParams = pl.listen(flag='wavFile', wavFP=fp) # read & analyze a WAV file\n",
    "    \n",
    "    flag, rsltTxt = pl.compareParamsOfSF2T(sfParams, tParams, path.basename(fp)) # compare analyzed parameters of the current WAV and template WAV\n",
    "    print(rsltTxt) # print output; this text is also recorded in the log file by 'compareParamsOfSF2T' function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with another template dataset\n",
    "\n",
    "This time, we load a different call type with the below line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSpAD, __, templP = pl.listen(flag='templateFolder', wavFP='input/sample_rfts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you run the code in 'Running comparison with WAV files in testing folder' again, the matching results should be different (matched wave files should be 'm_rapFTsik##.wav' files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing a WAV file of Mic. recording with template\n",
    "\n",
    "There is another function, *compareWAV2Template*.\n",
    "This function accespts a file path of a WAV file in its argument and treat this WAV file as a recording with a microphone. Therefore, it does not compare entire WAV file with template. Instead, it treats the WAV file as if it's streaming from microphone, capturing a sound fragment and comparing each captured sound with template data.\n",
    "\n",
    "The given test file, 'm_test.wav', has several alternating *phee* calls (five calls) and *rapid fire Tsik* calls (4 calls) with silent intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSpAD, __, templP = pl.listen(flag='templateFolder', wavFP='input/sample_phee') # make template with phee calls\n",
    "pl.compareWAV2Template('input/test/m_test.wav') # comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSpAD, __, templP = pl.listen(flag='templateFolder', wavFP='input/sample_rfts') # make template with tsik calls\n",
    "pl.compareWAV2Template('input/test/m_test.wav') # comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing a sound fragment from micropohone with template\n",
    "\n",
    "Running the below code will start listening via the first (pl.devIdx[0]) detected microphone.\n",
    "Then, it will start another thread to continous processing of captured sound (In the app, this part is done with wxPython's timer), which will compare audio data and print output text here and log file.\n",
    "\n",
    "\\* These threads will continuously work until you run the next code to end them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "pl.startContMicListening(pl.devIdx[0]) # start a thread to listen to microphone continuously.\n",
    "q2t = Queue()\n",
    "th = Thread(target=pl.contProcMicAudioData, args=(q2t,)) # continuous processing microphone audio data\n",
    "th.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the below code to finish the above threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pl.th != None: pl.endContMicListening() # end listening to mic.\n",
    "q2t.put(('msg', 'quit'), True, None) # end processing of mic. data\n",
    "th.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a new parameter in analysis\n",
    "\n",
    "To add a new parameter to be analyzed, follow below steps.\n",
    "\n",
    "1) Add a key string such as 'duration' in PyListener.**pKeys**.<br>\n",
    "If this new parameter should be used for the comparison, also conduct following steps.\n",
    "- If its value depends on loaded template WAV sounds,<br>&nbsp;&nbsp;&nbsp; add the same key string in PyListener.**compParamList**, and add a margin value to add/subtract in PyListener.**cplInitMargin**.<br>&nbsp;&nbsp;&nbsp; *e.g.*: Add a string item, 'duration', to PyListener.**compParamList** and add *duration_min=0.25* and *duration_max=0.5* to PyListener.**cplInitMargin** dictionary. (When the duration of a chosen template WAV file is one second, *pyListener* UI will show 0.75 - 1.5 seconds thresholding range for *duration*.)<br><br>Otherwise (such as *summedAmpRatio*),<br>&nbsp;&nbsp;&nbsp; add a string item, 'summedAmpRatio', in PyListener.**incCPL**, and add the initial threshold range in PyListener.**indCPRange**.<br>&nbsp;&nbsp;&nbsp; *e.g.*: Add a key, 'summedAmpRatio', to PyListener.**indCPL**, and add *summedAmpRatio_min=0.5* and *summedAmpRatio_max=2.0*. (Summed amplitude of a sound fragment can be in the range of half to twice of summed amplitude of template data.)<br><br>\n",
    "- Add a label in PyListener.**compParamLabel** to be appeared in UI of pyListener app.\n",
    "\n",
    "2) Add calculation lines for the new parameter in *analyzeSpectrogramArray* function in *PyListener*, and store the result value in *params[key]*, in which *key* is the same key string in PyListener.**pKeys**.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptions of items (of PyListener.compParamList), used to analyze sound data\n",
    "\n",
    "- **duration**: Duration of sound in seconds\n",
    "- **cmxN**: X coordinate value of center-of-mass of spectrogram image, normalized to 0.0. and 1.0.\n",
    "- **cmyN**: Y coordinate value of center-of-mass of spectrogram image, normalized to 0.0. and 1.0.\n",
    "- **avgNumDataInCol**: Average number of non-zero data points in columns of spectrogram. This number of data is calculated after cutting off too low and too high frequency data (defined as *PyListener.comp_freq_range*), then auto-contrasting on spectrogram.\n",
    "- **lowFreq**: Calculates lowest frequency in each column (also after cutting off and auto-contrasting). **lowFreq** is the average value of those lowest frequencies.\n",
    "- **highFreq**: High frequency counter-part of **lowFreq**.\n",
    "- **distLowRow2HighRow**: Distance (in pixels of spectrogram) between 'lowFreqRow' and 'highFreqRow'.\n",
    "- **summedAmpRatio**: 'summedAmp' of a captured sound fragment, divided by 'summedAmp' of template data\n",
    "- **corr2auto**: [correlate(s,t)/correlate(t,t)]. Correlation bewteen a captured sound fragment and template data, divided by auto-correlation of the template data. To calculate a single value, the maximum values of result arrays from correlation and auto-correlation are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
